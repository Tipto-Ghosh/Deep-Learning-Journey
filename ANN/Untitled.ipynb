{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc8dbfed-6aad-41f7-b1f7-ea5d3b45d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6b6fb94-118a-4665-94b8-adc27f7f870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[8,8,4],[7,9,5],[6,10,6],[5,12,7]], columns=['cgpa', 'profile_score', 'lpa'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf9e4b0-b50a-4a9c-af1b-46823688fd0b",
   "metadata": {},
   "source": [
    "## Making a neural network with 2 nodes in input layer , 1 hidden layer with 2 nodes and 1 node in output layer.\n",
    "\n",
    "---\n",
    "1. Neural Network Architecture:\n",
    "    - Inputs: 2 inputs x1 , x2\n",
    "    - Hidden Layer: 2 neurons -> h1 , h2\n",
    "    - Output Layer: y_hat\n",
    "---\n",
    "Input (2 features)<br>\n",
    "     <br>↓<br>\n",
    "Hidden Layer (2 neurons, linear activation) <br>\n",
    "     <br>↓<br>\n",
    "Output Layer (1 neuron, linear activation)<br>\n",
    "\n",
    "---\n",
    "2. Parameters:\n",
    "   - Weights:\n",
    "      - W1 : weights of layer-1, Shape (2 , 2) -> connects input to hidden layer.\n",
    "      - W2 : weights of layer-2(Hidden layer), Shape (2 , 1) -> connects hidden layer to output layer.\n",
    "    - Biases:\n",
    "      - b1: bias of layer-2(Hidden Layer), Shape (2 , 1)\n",
    "      - b2: bias of output layer\n",
    "        \n",
    "3. Forward Pass Equations:\n",
    "\n",
    "   Since all activations are linear:\n",
    "   $$h = W_1^T X + b_1$$\n",
    "   $$\\hat{y} = W_2^T h + b_2$$\n",
    "\n",
    "   Loss Function: The loss function (specifically, a mean squared error with a $\\frac{1}{2}$ coefficient) is:\n",
    "   $$L = \\frac{1}{2m} \\sum (y - \\hat{y})^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c2eb786-b83d-4406-b9cb-ffed57757d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_paramters(layer_dims): \n",
    "    \"\"\"\n",
    "    Function: initialize_parameters(layer_dims)\n",
    "    ------------------------------------------\n",
    "    Initializes all weights and biases for each layer.\n",
    "    \n",
    "    Arguments:\n",
    "    layer_dims -- list containing the number of neurons in each layer, \n",
    "                  including input and output layers. \n",
    "                  Example: [2, 2, 1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- dictionary containing:\n",
    "                  W1, b1, W2, b2\n",
    "                  Each key corresponds to a layer’s weights and biases.\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    parameters = {} \n",
    "    L = len(layer_dims)\n",
    "\n",
    "    # initalize all weights with 1 and bias with 0 for all layers. \n",
    "    for l in range(1 , L): # l = 1 -> weights if Input to Hidden Layer 1 and so on... Last l will be weights from Hidden(last) to Output\n",
    "        # initialize weights of current layer => numpy array with shape(no of neurons in previous layer , no of neurons in previous layer)\n",
    "        parameters['W' + str(l)] = np.ones(shape = (layer_dims[l] , layer_dims[l - 1])) \n",
    "        # initialize bias of current layer => shape(no of neurons in current layer , 1)\n",
    "        parameters['b' + str(l)] = np.zeros(shape = (layer_dims[l] , 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5fa7d4c-ecdd-4423-8462-28ae362779f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[1., 1.],\n",
       "        [1., 1.]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[1., 1.]]),\n",
       " 'b2': array([[0.]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing architecture\n",
    "layers = [2 , 2 , 1]\n",
    "parameters = initialize_paramters(layer_dims = layers)\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b7b9b8c-96f1-4ca8-87dc-e442178bc848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward_propagation(X , W , b):\n",
    "    \"\"\" \n",
    "    X: A single row(point)\n",
    "    W: weight matrix of a layer\n",
    "    b: bias of a layer.\n",
    "\n",
    "    Do the forward probagation using the linear activation function(y = mx + b) for a single point X.\n",
    "    \"\"\"\n",
    "    Z = np.dot(W , X) + b \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adf42e7e-608f-4e3b-a5ce-06677a9a9fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_forward_propagation(X , parameters): \n",
    "    \"\"\" \n",
    "    It performs forward propagation layer by layer.\n",
    "    It feeds input X forward through all layers, computing each layers outputs.\n",
    "    Formula: O(l) = Dot(W(l).T , O(l-1)) + b(l) \n",
    "\n",
    "    Returns:\n",
    "      - Final network output(y_hat)\n",
    "      - Previous layers activations(outputs). \n",
    "    \"\"\"\n",
    "    A = X # input layers activation is inputs itself. \n",
    "    L = len(parameters) // 2 # each layer has 2 pair of params weights and bias so no of layer is half of paramters. \n",
    "\n",
    "    # now starts from first hidden layer and go layer by layer.  \n",
    "    for l in range(1 , L + 1): \n",
    "        A_prev = A # activations from previous layer.(input if first layer)\n",
    "        # get the weights and bias of current layer to find the activations of current layer. \n",
    "        wl = parameters['W' + str(l)] # weights matrix of l-th layer. \n",
    "        bl = parameters['b' + str(l)] # bias vector of l-th layer. \n",
    "        # calculate activations for current layer l \n",
    "        A = linear_forward_propagation(X = A_prev , W = wl , b = bl) \n",
    "\n",
    "    # now A is our latest activation(final output) and A_prev = Activations from previous layer\n",
    "    return A , A_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6a0087e-649b-48b1-9a93-8b89cd0fb412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update paparameters\n",
    "def update_parameters(parameters , y , y_hat , A1 , X): \n",
    "    \"\"\" \n",
    "    Update all the parameters(weights and biases) of all layers from last to first layer(backward). \n",
    "    \"\"\"\n",
    "    lr = 0.001\n",
    "    # Output layer (layer 2)\n",
    "    parameters['W2'][0][0] -= lr * 2 * (y_hat - y) * A1[0][0]\n",
    "    parameters['W2'][1][0] -= lr * 2 * (y_hat - y) * A1[1][0]\n",
    "    parameters['b2'][0][0] -= lr * 2 * (y_hat - y)\n",
    "\n",
    "    # Hidden layer (layer 1)\n",
    "    parameters['W1'][0][0] -= lr * 2 * (y_hat - y) * parameters['W2'][0][0] * X[0][0]\n",
    "    parameters['W1'][0][1] -= lr * 2 * (y_hat - y) * parameters['W2'][0][0] * X[1][0]\n",
    "    parameters['b1'][0][0] -= lr * 2 * (y_hat - y) * parameters['W2'][0][0] \n",
    "\n",
    "    # Input layer \n",
    "    parameters['W1'][1][0] -= lr * 2 * (y_hat - y) * parameters['W2'][1][0] * X[0][0]\n",
    "    parameters['W1'][1][1] -= lr * 2 * (y_hat - y) * parameters['W2'][1][0] * X[1][0]\n",
    "    parameters['b1'][1][0] -= lr * 2 * (y_hat - y) * parameters['W2'][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cccd3a8-418a-4d02-ba68-76768becf3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32. 32.]]\n",
      "[[32. 32.]]\n",
      "[[32. 32.]]\n",
      "[[34. 34.]]\n",
      "[[32. 32.]]\n",
      "[[32. 32.]]\n",
      "[[32. 32.]]\n",
      "[[34. 34.]]\n",
      "[[32. 32.]]\n",
      "[[32. 32.]]\n",
      "[[32. 32.]]\n",
      "[[34. 34.]]\n",
      "[[32. 32.]]\n",
      "[[32. 32.]]\n",
      "[[32. 32.]]\n",
      "[[34. 34.]]\n",
      "[[32. 32.]]\n",
      "[[32. 32.]]\n",
      "[[32. 32.]]\n",
      "[[34. 34.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[1., 1.],\n",
       "        [1., 1.]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[1., 1.]]),\n",
       " 'b2': array([[0.]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = initialize_paramters([2 , 2 , 1])\n",
    "epochs = 5\n",
    "\n",
    "for i in range(epochs): \n",
    "    loss = []\n",
    "    \n",
    "    for j in range(df.shape[0]): \n",
    "        X = df[['cgpa' , 'profile_score']].values[j]   \n",
    "        y = df[['lpa']].values[j][0]\n",
    "\n",
    "        # get the prediction and activation values of all layers \n",
    "        y_hat , A1 = L_layer_forward_propagation(X , parameters)\n",
    "        print(y_hat)\n",
    "    #     y_hat = y_hat[0][0]\n",
    "\n",
    "    #     update_parameters(parameters , y , y_hat , A1 , X) \n",
    "    #     loss.append((y - y_hat) ** 2)\n",
    "    \n",
    "    # print('Epoch - ',i+1,'Loss - ',np.array(Loss).mean())\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa977e1-d06e-487a-80ea-7ce9144a1cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f1d532b-2834-437d-ba8b-a9775e80b653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 8])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['cgpa' , 'profile_score']].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e990e427-3871-4dff-ab6a-dc54775842e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1512f051-9b98-42f0-bdf9-445f2f482a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 6, 7])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lpa'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1e6e60c-ba02-445e-b02c-5a218012cd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lpa'].values[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1ecf17-846f-4350-9dba-2abc6e92e8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL311)",
   "language": "python",
   "name": "dl311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
