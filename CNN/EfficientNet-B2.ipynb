{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "530499e4-1b33-4135-8285-91e12bc89dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb1def4-3ba5-47ae-8d9f-1133cfd2a496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96bbf407-a400-4883-b1be-8d5464eb20e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNet-B2 expects 260×260 images. \n",
    "IMG_SIZE = 260\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a4c493d-c93d-426a-808c-e907d3beb5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train transforms \n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "066617df-0ecb-4dd0-8630-59f99f61e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test and validation transforms \n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "594dc704-c258-4dd1-8b05-241fc2ca4e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"Data/snack_vision_lite/train\"\n",
    "val_dir = \"Data/snack_vision_lite/val\"\n",
    "test_dir = \"Data/snack_vision_lite/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c8dc804-5d0b-4f19-8f6f-de8d5a0f31c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(train_dir , train_transforms)\n",
    "test_dataset = datasets.ImageFolder(test_dir , val_transforms)\n",
    "val_dataset = datasets.ImageFolder(val_dir , val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6de6bee3-37f9-4035-8390-57b76ed51bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset , batch_size = BATCH_SIZE , pin_memory = True , num_workers = 4 , shuffle = True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset = val_dataset , batch_size = BATCH_SIZE , pin_memory = True , num_workers = 4\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset = test_dataset , batch_size = BATCH_SIZE , pin_memory = True , num_workers = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c11ee488-f753-4834-9e44-405136bef716",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "704b60a0-faf9-40ee-ae18-fbde25f02d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d0abb8c-6f2f-4037-9582-b24df29f7cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.efficientnet_b2(weights = \"IMAGENET1K_V1\" , progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3f72a93-2d31-4bc3-bbf8-2c075360e692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "EfficientNet                                            [32, 1000]                --\n",
       "├─Sequential: 1-1                                       [32, 1408, 9, 9]          --\n",
       "│    └─Conv2dNormActivation: 2-1                        [32, 32, 130, 130]        --\n",
       "│    │    └─Conv2d: 3-1                                 [32, 32, 130, 130]        864\n",
       "│    │    └─BatchNorm2d: 3-2                            [32, 32, 130, 130]        64\n",
       "│    │    └─SiLU: 3-3                                   [32, 32, 130, 130]        --\n",
       "│    └─Sequential: 2-2                                  [32, 16, 130, 130]        --\n",
       "│    │    └─MBConv: 3-4                                 [32, 16, 130, 130]        1,448\n",
       "│    │    └─MBConv: 3-5                                 [32, 16, 130, 130]        612\n",
       "│    └─Sequential: 2-3                                  [32, 24, 65, 65]          --\n",
       "│    │    └─MBConv: 3-6                                 [32, 24, 65, 65]          6,004\n",
       "│    │    └─MBConv: 3-7                                 [32, 24, 65, 65]          10,710\n",
       "│    │    └─MBConv: 3-8                                 [32, 24, 65, 65]          10,710\n",
       "│    └─Sequential: 2-4                                  [32, 48, 33, 33]          --\n",
       "│    │    └─MBConv: 3-9                                 [32, 48, 33, 33]          16,518\n",
       "│    │    └─MBConv: 3-10                                [32, 48, 33, 33]          43,308\n",
       "│    │    └─MBConv: 3-11                                [32, 48, 33, 33]          43,308\n",
       "│    └─Sequential: 2-5                                  [32, 88, 17, 17]          --\n",
       "│    │    └─MBConv: 3-12                                [32, 88, 17, 17]          50,300\n",
       "│    │    └─MBConv: 3-13                                [32, 88, 17, 17]          123,750\n",
       "│    │    └─MBConv: 3-14                                [32, 88, 17, 17]          123,750\n",
       "│    │    └─MBConv: 3-15                                [32, 88, 17, 17]          123,750\n",
       "│    └─Sequential: 2-6                                  [32, 120, 17, 17]         --\n",
       "│    │    └─MBConv: 3-16                                [32, 120, 17, 17]         149,158\n",
       "│    │    └─MBConv: 3-17                                [32, 120, 17, 17]         237,870\n",
       "│    │    └─MBConv: 3-18                                [32, 120, 17, 17]         237,870\n",
       "│    │    └─MBConv: 3-19                                [32, 120, 17, 17]         237,870\n",
       "│    └─Sequential: 2-7                                  [32, 208, 9, 9]           --\n",
       "│    │    └─MBConv: 3-20                                [32, 208, 9, 9]           301,406\n",
       "│    │    └─MBConv: 3-21                                [32, 208, 9, 9]           686,868\n",
       "│    │    └─MBConv: 3-22                                [32, 208, 9, 9]           686,868\n",
       "│    │    └─MBConv: 3-23                                [32, 208, 9, 9]           686,868\n",
       "│    │    └─MBConv: 3-24                                [32, 208, 9, 9]           686,868\n",
       "│    └─Sequential: 2-8                                  [32, 352, 9, 9]           --\n",
       "│    │    └─MBConv: 3-25                                [32, 352, 9, 9]           846,900\n",
       "│    │    └─MBConv: 3-26                                [32, 352, 9, 9]           1,888,920\n",
       "│    └─Conv2dNormActivation: 2-9                        [32, 1408, 9, 9]          --\n",
       "│    │    └─Conv2d: 3-27                                [32, 1408, 9, 9]          495,616\n",
       "│    │    └─BatchNorm2d: 3-28                           [32, 1408, 9, 9]          2,816\n",
       "│    │    └─SiLU: 3-29                                  [32, 1408, 9, 9]          --\n",
       "├─AdaptiveAvgPool2d: 1-2                                [32, 1408, 1, 1]          --\n",
       "├─Sequential: 1-3                                       [32, 1000]                --\n",
       "│    └─Dropout: 2-10                                    [32, 1408]                --\n",
       "│    └─Linear: 2-11                                     [32, 1000]                1,409,000\n",
       "=========================================================================================================\n",
       "Total params: 9,109,994\n",
       "Trainable params: 9,109,994\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 31.80\n",
       "=========================================================================================================\n",
       "Input size (MB): 25.96\n",
       "Forward/backward pass size (MB): 7078.21\n",
       "Params size (MB): 36.44\n",
       "Estimated Total Size (MB): 7140.61\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model , input_size = (BATCH_SIZE , 3 , IMG_SIZE , IMG_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e78ce43f-22c8-4e16-8e7a-46bf72b026cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetUpdated(nn.Module): \n",
    "    def __init__(self , base_model, num_classes, freeze_backbone = True, dropout = 0.3): \n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = base_model\n",
    "\n",
    "        # -------freeze the cnn part(backbone) ----------\n",
    "        if freeze_backbone: \n",
    "            for param in self.base_model.parameters(): \n",
    "                param.requires_grad = False\n",
    "\n",
    "        # ------ replace classifier with our objective -------------\n",
    "        if hasattr(self.base_model , 'classifier'): \n",
    "            in_features = self.base_model.classifier[-1].in_features\n",
    "            # replace now \n",
    "            self.base_model.classifier = nn.Sequential(\n",
    "                nn.Dropout(dropout), \n",
    "                nn.Linear(in_features , num_classes)\n",
    "            )\n",
    "        elif hasattr(self.base_model , 'fc'): \n",
    "            in_features = self.base_model.fc.in_features\n",
    "            self.base_model.fc = nn.Sequential(\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(in_features, num_classes)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model architecture\")\n",
    "            \n",
    "    def forward(self , x): \n",
    "        return self.base_model(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9696c50d-8afa-405d-b17e-f61ee5115104",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_model = EfficientNetUpdated(base_model = model,num_classes = num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a000b3d7-aefc-4c92-b4d2-d9f4504f6bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, updated_model.parameters()), lr = 1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "120d6adb-1647-4e99-860b-a9d25358af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_epoch(model,train_loader,val_loader, criterion, optimizer, device):\n",
    "    # -------- Training --------\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # -------- Training accuracy --------\n",
    "    model.eval()\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            train_correct += (preds == y).sum().item()\n",
    "            total_train += y.size(0)\n",
    "\n",
    "    train_acc = train_correct / total_train\n",
    "\n",
    "    # -------- Validation accuracy --------\n",
    "    val_correct = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            val_correct += (preds == y).sum().item()\n",
    "            total_val += y.size(0)\n",
    "\n",
    "    val_acc = val_correct / total_val\n",
    "\n",
    "    return train_loss, train_acc, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc2c128-eaa9-496f-8940-7a9a726d6203",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS): \n",
    "    train_loss, train_acc, val_acc = train_single_epoch(\n",
    "        model = updated_model, \n",
    "        train_loader = train_loader,optimizer = optimizer,\n",
    "        val_loader = val_loader,criterion = criterion,device = device\n",
    "    )\n",
    "    print(f\"[{epoch + 1}/{EPOCHS}]: train loss: {train_loss:.4f} | train acc: {train_acc:.4f} | val acc: {val_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
